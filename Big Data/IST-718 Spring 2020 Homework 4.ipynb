{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Willard Williamson <wewillia@syr.edu>\n",
    "- Faculty Assistant: Yash Pasar <yspasar@syr.edu>\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- There could be tests in some cells (i.e., `assert` and `np.testing.` statements). These tests (if present) are used to grade your answers. **However, the professor and FAs could use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Kernel`$\\rightarrow$`Restart and Run All`.  All runtime errors will result in a minimum penalty of half off.\n",
    "- Data Bricks is the official class runtime environment so you should test your code on Data Bricks before submission.  If there is a runtime problem in the grading environment, we will try your code on Data Bricks before making a final grading decision.\n",
    "- All plots shall include a title, and axis labels.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
    "- Don't add or remove files from your git repo.\n",
    "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
    "- You are free to add additional code cells around the cells marked `your code here`.\n",
    "- Students may use toPandas() to print the head of data frames.\n",
    "- __Only use spark, spark machine learning, spark data frames, RDD's, and map reduce to solve all problems unless instructed otherwise.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Using the get_training_filename function defined in the cell above, read the sms_spam.csv file into a spark dataframe named spam_df.  There should be no empty columns in spam_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|type|                text|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if that's th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#read sms_spam.csv file into a spark dataframe named spam_df\n",
    "spam_df = spark.read.csv(get_training_filename('sms_spam.csv'), header=True, inferSchema=True)\n",
    "spam_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Starting with spam_df, create a new dataframe named spam_df1.  Rename the spam_df type column to be named spam.  In the spam column, replace the string `spam` the with the integer 1 and the string `ham` with the integer 0.  Print the head and shape of spam_df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|SPAMM|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if that's th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------------------+\n",
      "|spam|                text|\n",
      "+----+--------------------+\n",
      "|   0|Go until jurong p...|\n",
      "|   0|Ok lar... Joking ...|\n",
      "|   1|Free entry in 2 a...|\n",
      "|   0|U dun say so earl...|\n",
      "|   0|Nah I don't think...|\n",
      "|   1|FreeMsg Hey there...|\n",
      "|   0|Even my brother i...|\n",
      "|   0|As per your reque...|\n",
      "|   1|WINNER!! As a val...|\n",
      "|   1|Had your mobile 1...|\n",
      "|   0|I'm gonna be home...|\n",
      "|   1|SIX chances to wi...|\n",
      "|   1|URGENT! You have ...|\n",
      "|   0|I've been searchi...|\n",
      "|   0|I HAVE A DATE ON ...|\n",
      "|   1|XXXMobileMovieClu...|\n",
      "|   0|Oh k...i'm watchi...|\n",
      "|   0|Eh u remember how...|\n",
      "|   0|Fine if that's th...|\n",
      "|   1|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Head of spam_df1: Row(spam=0, text='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')\n",
      "\n",
      "\n",
      "Shape of spam_df1: 5574 2\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "#create spam_df1 and rename 'type' to 'spam'\n",
    "spam_df1 = spam_df.withColumnRenamed(\"type\", \"SPAMM\") #column name changed to SPAMM to avoid confusion when \n",
    "                                                      #dropping extra columns later                      \n",
    "spam_df1.show()\n",
    "\n",
    "#in spam column, replacing 'spam' with 1 and 'ham' with 0\n",
    "from pyspark.sql import functions as fn\n",
    "    \n",
    "binary = spam_df1.select(\"SPAMM\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "Expr = [fn.when(fn.col(\"SPAMM\") == spam, 1).otherwise(0).alias(spam) for spam in binary]\n",
    "spam_df1 = spam_df1.select(Expr + spam_df1.columns)\n",
    "#spam_df1.show()\n",
    "\n",
    "#dropping the extra columns\n",
    "spam_df1 = spam_df1.drop(\"ham\")\n",
    "spam_df1 = spam_df1.drop(\"SPAMM\")\n",
    "spam_df1.show()\n",
    "\n",
    "#head and shape of spam_df1\n",
    "print(\"Head of spam_df1:\", spam_df1.head())\n",
    "print(\"\\n\")\n",
    "print(\"Shape of spam_df1:\", spam_df1.count(), len(spam_df1.columns)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Starting with spam_df1, create a new dataframe named spam_df2 with a new column named filtered_text by removing stop words from the text column in spam_df.  Print the head and shape of spam_df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|                text|     tokenized_words|       filtered_text|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|   0|Go until jurong p...|[go, until, juron...|[jurong, point,, ...|\n",
      "|   0|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|\n",
      "|   1|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|\n",
      "|   0|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|\n",
      "|   0|Nah I don't think...|[nah, i, don't, t...|[nah, don't, thin...|\n",
      "|   1|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|\n",
      "|   0|Even my brother i...|[even, my, brothe...|[brother, like, s...|\n",
      "|   0|As per your reque...|[as, per, your, r...|[request, 'melle,...|\n",
      "|   1|WINNER!! As a val...|[winner!!, as, a,...|[winner!!, valued...|\n",
      "|   1|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|\n",
      "|   0|I'm gonna be home...|[i'm, gonna, be, ...|[i'm, gonna, home...|\n",
      "|   1|SIX chances to wi...|[six, chances, to...|[chances, win, ca...|\n",
      "|   1|URGENT! You have ...|[urgent!, you, ha...|[urgent!, won, 1,...|\n",
      "|   0|I've been searchi...|[i've, been, sear...|[i've, searching,...|\n",
      "|   0|I HAVE A DATE ON ...|[i, have, a, date...|[date, sunday, wi...|\n",
      "|   1|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|\n",
      "|   0|Oh k...i'm watchi...|[oh, k...i'm, wat...|[oh, k...i'm, wat...|\n",
      "|   0|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|\n",
      "|   0|Fine if that's th...|[fine, if, that's...|[fine, that's, wa...|\n",
      "|   1|England v Macedon...|[england, v, mace...|[england, v, mace...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Head of spam_df2: Row(spam=0, text='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', tokenized_words=['go', 'until', 'jurong', 'point,', 'crazy..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'there', 'got', 'amore', 'wat...'], filtered_text=['jurong', 'point,', 'crazy..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'got', 'amore', 'wat...'])\n",
      "\n",
      "\n",
      "Shape of spam_df2: 5574 4\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer().setInputCol('text').setOutputCol('tokenized_words')\n",
    "\n",
    "\n",
    "spam_df2 = tokenizer.transform(spam_df1)\n",
    "#spam_df2.show(5)\n",
    "\n",
    "\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "stop_words[0:10]\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw = StopWordsRemover().setStopWords(stop_words).setCaseSensitive(False).setInputCol(\"tokenized_words\")\\\n",
    "        .setOutputCol(\"filtered_text\")\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "sw_pipeline = Pipeline(stages = [tokenizer, sw]).fit(spam_df1)\n",
    "\n",
    "\n",
    "spam_df2 = sw_pipeline.transform(spam_df1)\n",
    "spam_df2.show()\n",
    "\n",
    "\n",
    "#head and shape of spam_df1\n",
    "print(\"Head of spam_df2:\", spam_df2.head())\n",
    "print(\"\\n\")\n",
    "print(\"Shape of spam_df2:\", spam_df2.count(), len(spam_df2.columns)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Create a new dataframe named spam_df3 starting with spam_df2.  Create a new column named tfidf by performing a term frequency / inverse document frequency transformation on the filtered_text column of spam_df2.<br>  \n",
    "\n",
    "- Print the head and shape of spam_df3.  \n",
    "- Print the top 10 most important words indicated by the TFIDF score.  \n",
    "- Print the 10 least important words as indicated by the TFIDF score.\n",
    "- Print the total number of columns in the TFIDF data in spam_df3\n",
    "- Print the number of rows in the TFIDF data in spam_df3\n",
    "- Based only on the number of rows and columns in the TFIDF data, do you expect the model to overfit.  Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|spam|                text|     tokenized_words|       filtered_text|                  tf|               tfidf|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   0|Go until jurong p...|[go, until, juron...|[jurong, point,, ...|(13262,[10,29,56,...|(13262,[10,29,56,...|\n",
      "|   0|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|(13262,[0,23,268,...|(13262,[0,23,268,...|\n",
      "|   1|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(13262,[1,12,18,2...|(13262,[1,12,18,2...|\n",
      "|   0|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(13262,[0,64,71,1...|(13262,[0,64,71,1...|\n",
      "|   0|Nah I don't think...|[nah, i, don't, t...|[nah, don't, thin...|(13262,[30,35,279...|(13262,[30,35,279...|\n",
      "|   1|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(13262,[9,47,60,1...|(13262,[9,47,60,1...|\n",
      "|   0|Even my brother i...|[even, my, brothe...|[brother, like, s...|(13262,[9,48,220,...|(13262,[9,48,220,...|\n",
      "|   0|As per your reque...|[as, per, your, r...|[request, 'melle,...|(13262,[167,386,8...|(13262,[167,386,8...|\n",
      "|   1|WINNER!! As a val...|[winner!!, as, a,...|[winner!!, valued...|(13262,[42,104,12...|(13262,[42,104,12...|\n",
      "|   1|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|(13262,[0,12,27,3...|(13262,[0,12,27,3...|\n",
      "|   0|I'm gonna be home...|[i'm, gonna, be, ...|[i'm, gonna, home...|(13262,[3,17,30,3...|(13262,[3,17,30,3...|\n",
      "|   1|SIX chances to wi...|[six, chances, to...|[chances, win, ca...|(13262,[7,16,34,8...|(13262,[7,16,34,8...|\n",
      "|   1|URGENT! You have ...|[urgent!, you, ha...|[urgent!, won, 1,...|(13262,[12,28,42,...|(13262,[12,28,42,...|\n",
      "|   0|I've been searchi...|[i've, been, sear...|[i've, searching,...|(13262,[87,88,205...|(13262,[87,88,205...|\n",
      "|   0|I HAVE A DATE ON ...|[i, have, a, date...|[date, sunday, wi...|(13262,[510,1571,...|(13262,[510,1571,...|\n",
      "|   1|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(13262,[28,98,156...|(13262,[28,98,156...|\n",
      "|   0|Oh k...i'm watchi...|[oh, k...i'm, wat...|[oh, k...i'm, wat...|(13262,[73,192,45...|(13262,[73,192,45...|\n",
      "|   0|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(13262,[0,1,45,11...|(13262,[0,1,45,11...|\n",
      "|   0|Fine if that's th...|[fine, if, that's...|[fine, that's, wa...|(13262,[0,67,94,9...|(13262,[0,67,94,9...|\n",
      "|   1|England v Macedon...|[england, v, mace...|[england, v, mace...|(13262,[4,28,32,5...|(13262,[4,28,32,5...|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Head of spam_df3: Row(spam=0, text='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', tokenized_words=['go', 'until', 'jurong', 'point,', 'crazy..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'there', 'got', 'amore', 'wat...'], filtered_text=['jurong', 'point,', 'crazy..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'got', 'amore', 'wat...'], tf=SparseVector(13262, {10: 1.0, 29: 1.0, 56: 1.0, 63: 1.0, 303: 1.0, 581: 1.0, 722: 1.0, 1337: 1.0, 1797: 1.0, 4348: 1.0, 6309: 1.0, 8525: 1.0, 8670: 1.0, 10034: 1.0, 12820: 1.0}), tfidf=SparseVector(13262, {10: 3.2099, 29: 3.8469, 56: 4.2072, 63: 4.322, 303: 5.4072, 581: 5.918, 722: 6.1411, 1337: 6.6801, 1797: 6.8343, 4348: 7.5274, 6309: 7.9329, 8525: 7.9329, 8670: 7.9329, 10034: 7.9329, 12820: 7.9329}))\n",
      "\n",
      "\n",
      "Shape of spam_df3: 5574 6\n",
      "\n",
      "\n",
      "Top 10 most important words as indicated by the TFIDF score is:\n",
      " ['bird!', 'deals', 'mns', 'treated?', 'slices', 'companion', '2nite-tell', 'absolutely', 'ou', 'young']\n",
      "\n",
      "\n",
      "Respective IDF scores:\n",
      " [7.93290042 7.93290042 7.93290042 7.93290042 7.93290042 7.93290042\n",
      " 7.93290042 7.93290042 7.93290042 7.93290042]\n",
      "\n",
      "\n",
      "Top 10 least important words as indicated by the TFIDF score is:\n",
      " ['2', '', \"i'm\", 'ur', 'just', '&lt;#&gt;', '4', '.', 'like', 'got']\n",
      "\n",
      "\n",
      "Respective IDF scores:\n",
      " [2.71254459 3.05770309 2.77097567 2.90246249 2.75392981 3.25540957\n",
      " 3.16646208 3.82202655 3.23242005 3.20994719]\n",
      "\n",
      "\n",
      "Total number of columns in spam_df3: 13262\n",
      "\n",
      "\n",
      "Total number of rows in spam_df3: 5574\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import feature, classification, evaluation, Pipeline\n",
    "\n",
    "\n",
    "cv = CountVectorizer().setInputCol(\"filtered_text\").setOutputCol(\"tf\")\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "\n",
    "\n",
    "tfidf_pipeline = Pipeline(stages = [cv, idf]).fit(spam_df2)\n",
    "\n",
    "\n",
    "spam_df3 = tfidf_pipeline.transform(spam_df2)\n",
    "spam_df3.show()\n",
    "\n",
    "\n",
    "#head and shape of spam_df1\n",
    "print(\"Head of spam_df3:\", spam_df3.head())\n",
    "print(\"\\n\")\n",
    "print(\"Shape of spam_df3:\", spam_df3.count(), len(spam_df3.columns)) \n",
    "print(\"\\n\")\n",
    "\n",
    "#top 10 most important words indicated by the TFIDF score\n",
    "x = tfidf_pipeline.stages[0].vocabulary\n",
    "y = tfidf_pipeline.stages[1].idf\n",
    "topTen = y[-10:]\n",
    "topTenAgain = x[-10:]\n",
    "print(\"Top 10 most important words as indicated by the TFIDF score is:\\n\", topTenAgain)\n",
    "print(\"\\n\")\n",
    "print(\"Respective IDF scores:\\n\", topTen)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#10 least important words as indicated by the TFIDF score\n",
    "print(\"Top 10 least important words as indicated by the TFIDF score is:\\n\", x[1:11])\n",
    "print(\"\\n\")\n",
    "print(\"Respective IDF scores:\\n\", y[1:11])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#total number of columns in the TFIDF data in spam_df3\n",
    "print(\"Total number of columns in spam_df3:\", len(y))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#number of rows in the TFIDF data in spam_df3\n",
    "print(\"Total number of rows in spam_df3:\", spam_df3.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#Since the number of features are more as compared to the number of rows, the model will overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model overfit explanation here:\n",
    "\n",
    "Since the number of features are more as compared to the number of rows, the model will overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Create a pipeline named pipe1 capable of predicting ham or spam using logistic regression using spam_df3 as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "from pyspark.ml import feature, classification, evaluation, Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "training_df, validation_df, testing_df = spam_df3.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "\n",
    "\n",
    "lr = LogisticRegression().setLabelCol(\"spam\").setFeaturesCol(\"tfidf\").setRegParam(0.0).setMaxIter(100).\\\n",
    "        setElasticNetParam(0.)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline(stages=[lr]).fit(spam_df3)\n",
    "spam_df3 = spam_df3.withColumn(\"spam\", spam_df3[\"spam\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Fit pipe1 using a [CrossValidator](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) object with the number of cross validation folds = 3.  Score the model using a [BinaryClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html) using ROC AUC as the metric.  Name the cross validator object cv1 and the fitted cross validator object fitted_cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = \"spam\")\n",
    "\n",
    "\n",
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.2]).addGrid(lr.elasticNetParam, [0.2]).build())\n",
    "\n",
    "\n",
    "cv1 = CrossValidator(estimator = lr, estimatorParamMaps = paramGrid, evaluator = evaluator, numFolds = 3)\n",
    "\n",
    "\n",
    "fitted_cv1 = cv1.fit(testing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Print the cross validation AUC score from fitted_cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9637503823799327"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "evaluator.evaluate(fitted_cv1.transform(testing_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Create a ROC scatter plot from fitted_pipe1 TPR/FPR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TPR\n",
      "0  0.000000\n",
      "1  0.078983\n",
      "2  0.153949\n",
      "3  0.236948\n",
      "4  0.319946\n",
      "   FPR\n",
      "0  0.0\n",
      "1  0.0\n",
      "2  0.0\n",
      "3  0.0\n",
      "4  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fpr = pipe1.stages[-1].summary.roc.select(\"TPR\").toPandas()\n",
    "tpr = pipe1.stages[-1].summary.roc.select(\"FPR\").toPandas()\n",
    "\n",
    "\n",
    "print(fpr.head())\n",
    "print(tpr.head())\n",
    "\n",
    "\n",
    "plt.scatter(tpr, fpr)\n",
    "plt.title(\"ROC scatter Plot for TPR v/s FPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Create a new cross validator object named cv2 similar to cv1 but this time add a ParamGridBuilder.  Define a grid of elastic net regularization parameters. Fit cv2 and name the resulting fitted cross validator fitted_cv2.  The number of parameters in your grid should be limited such that it runs in a reasonable amount of time (around 5 to 10 minutes max).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "ParaGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.3, 0.5]).addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3]) \n",
    "             .build())\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = \"spam\")\n",
    "\n",
    "\n",
    "cv2 = CrossValidator(estimator = lr, estimatorParamMaps = ParaGrid, evaluator = evaluator, numFolds = 3)\n",
    "\n",
    "\n",
    "fitted_cv2 = cv2.fit(testing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "- Print the resulting AUC from fitted_cv2. \n",
    "- Print the best model's L1 and L2 regularization parameters\n",
    "- Analyze the L1 feature selection:\n",
    "    - Print the total number of features\n",
    "    - Print the number of features that L1 regularization eliminated\n",
    "    - If any features were eliminated, print a sample of 10 words that were eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score from fitted_cv2 0.9707072194585952\n",
      "\n",
      "\n",
      "L1 is  0.1\n",
      "L2 is  0.9\n",
      "\n",
      "\n",
      "Total number of features: 13262\n",
      "Number of features eleminated by L1 regularization: 12518\n",
      "Sample of words eliminated by L1 regularisation: ['', '!', '!!', '!!!', '!!!!', \"!!''.\", '!1', '!:-)', '!this', '\"\"']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "print(\"AUC score from fitted_cv2\", evaluator.evaluate(fitted_cv2.transform(spam_df3)))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "x1 = fitted_cv2.bestModel._java_obj.getElasticNetParam()\n",
    "x2 = 1 - x1\n",
    "\n",
    "\n",
    "print(\"L1 is \", format(x1))\n",
    "print(\"L2 is \", format(x2))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "no_of_features = pd.Series(OrderedDict(sorted((zip(tfidf_pipeline.stages[0].vocabulary,fitted_cv2.bestModel\\\n",
    "                                                   .coefficients.toArray())))))\n",
    "\n",
    "\n",
    "print(\"Total number of features:\", len(no_of_features))\n",
    "print(\"Number of features eleminated by L1 regularization:\", format(len(no_of_features[no_of_features == 0])))\n",
    "print(\"Sample of words eliminated by L1 regularisation:\",list(no_of_features[:10].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "Analyze the best model weights in fitted_cv2.  Print the 10 words that contribute the most to predicting spam.  Print the 10 words that contribute the least to predicting spam.  Do the words make sense?  Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten words that contribute the most to predicting spam: Index(['ü.', 'ü..', 'ü...', 'ü?', '–', '“', '“harry', '…', '….', '…thanks'], dtype='object')\n",
      "\n",
      "\n",
      "Ten words that contribute the least to predicting spam: ['', '!', '!!', '!!!', '!!!!', \"!!''.\", '!1', '!:-)', '!this', '\"\"']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "print(\"Ten words that contribute the most to predicting spam:\", format(no_of_features[-10:].index))\n",
    "print(\"\\n\")\n",
    "print(\"Ten words that contribute the least to predicting spam:\", list(no_of_features[:10].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit (5 pts)**  This question is optional.  If you choose to answer this question, you will earn 5 extra credit points.  If you choose not to answer this question, no points will be deducted from your score.  Solve the following equation for $c$ symbolically using the python sympy package.  Convert the solved symbolic solution to a latex format (this can be done with a pyton call), then populate the solution cell with the resulting latex code so that your solution shows up symbolically similar the equation below.\n",
    "\n",
    "$$c g - c h + e \\left(a + 1\\right)^{b} - \\frac{d \\left(\\left(a + 1\\right)^{b} - 1\\right)}{a} + \\frac{f \\left(\\left(a + 1\\right)^{b} - 1\\right)}{a} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete or change this cell\n",
    "\n",
    "# if running on data bricks\n",
    "if is_databricks():\n",
    "    # install sympy\n",
    "    dbutils.library.installPyPI\n",
    "    dbutils.library.installPyPI('sympy')\n",
    "    print(dbutils.library.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[ \\frac{- a e \\left(a + 1\\right)^{b} + d \\left(a + 1\\right)^{b} - d - f \\left(a + 1\\right)^{b} + f}{a \\left(g - h\\right)}\\right]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import sympy as sym\n",
    "\n",
    "\n",
    "a = sym.Symbol('a')\n",
    "b = sym.Symbol('b')\n",
    "c = sym.Symbol('c')\n",
    "d = sym.Symbol('d')\n",
    "e = sym.Symbol('e')\n",
    "f = sym.Symbol('f')\n",
    "g = sym.Symbol('g')\n",
    "h = sym.Symbol('h')\n",
    "\n",
    "\n",
    "x = c*g - c*h\n",
    "y = (e*(a+1)**b)\n",
    "z = ((d*((a+1)**b-1))/a)\n",
    "w = ((f*((a+1)**b-1)/a))\n",
    "     \n",
    "eqn = sym.solve(x+y-z+w,c)\n",
    "     \n",
    "print(sym.latex(eqn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your latex output here such that a human readable equation is displayed for grading\n",
    "$$\\left[ \\frac{- a e \\left(a + 1\\right)^{b} + d \\left(a + 1\\right)^{b} - d - f \\left(a + 1\\right)^{b} + f}{a \\left(g - h\\right)}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
